%                                                                 aa.dem
% AA vers. 8.2, LaTeX class for Astronomy & Astrophysics
% demonstration file
%                                                       (c) EDP Sciences
%-----------------------------------------------------------------------
%
%\documentclass[referee]{aa} % for a referee version
%\documentclass[onecolumn]{aa} % for a paper on 1 column  
%\documentclass[longauth]{aa} % for the long lists of affiliations 
%\documentclass[rnote]{aa} % for the research notes
%\documentclass[letter]{aa} % for the letters 
%\documentclass[bibyear]{aa} % if the references are not structured 
% according to the author-year natbib style

%
\documentclass{aa}  

%
\usepackage{graphicx}
\usepackage{txfonts}
%\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage{amsmath}	
%\usepackage{amssymb}
%\usepackage[colorinlistoftodos]{todonotes}
%\usepackage[square, numbers, comma, sort&compress]{natbib}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
\usepackage{textcomp}
\usepackage{courier}
\usepackage{siunitx}
\usepackage{tabularx}
\usepackage{pdflscape}
%\usepackage{afterpage}
\usepackage{capt-of}% or use the larger `caption` package
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}
\def\memohr#1{\color{blue}$HR[${\bf #1}$]$ \color{black}}
\def\memodt#1{\color{green}$DT[${\bf #1}$]$ \color{black}}
\def\memorl#1{\color{gray}$RL[${\bf #1}$]$ \color{black}}
\newcommand{\reb}{{\sc \tt REBOUND}\xspace}

%\usepackage[options]{hyperref}
% To add links in your PDF file, use the package "hyperref"
% with options according to your LaTeX or PDFLaTeX drivers.
%
\begin{document} 


   \title{Using Langevin Based MCMC in Exoplanet Radial Velocity Surveys}

   \author{R. Leblanc
          \inst{1,3}
          \and
          H. Rein\inst{2,3}
          \and 
          E. B. Ford\inst{4,5,6}
          \and 
          B. E. Nelson\inst{4,5,6}
          }

   \institute{Department of Physics, University of Toronto, Toronto, Ontario M5S 1A7, Canada
   \and
   Department of Physical and Environmental Sciences, University of Toronto at Scarborough, Toronto, Ontario M1C 1A4, Canada
   \and 
   Department of Astronomy and Astrophysics, University of Toronto, Toronto, Ontario M5S 3H4, Canada
   \and 
   Center for Exoplanets and Habitable Worlds, The Pennsylvania State University 525 Davey Laboratory, University Park, PA, 16802, USA
   \and 
   Department of Astronomy and Astrophysics, The Pennsylvania State University 525 Davey Laboratory, University Park, PA, 16802, USA
   \and 
   Department of Astronomy, University of Florida, 211 Bryant Space Science Center, Gainesville, FL 32611, USA
             }

   \date{Draft - April 12 2017
       \memohr{A few general notes:\\
           1) AandA charges publication fees. MNRAS does not. For that reason I like MNRAS.\\
           2) Did you choose the abstract with subheadings on purpose? This kind of abstract is not universally appreciated in the community. \\
           3) In tex files, I find it useful to have one line per sentence. This makes it easy to move sentences around. It also helps git to make reasonable diffs.\\
       }
       \memorl{ RE: 1) I will move this work to an MNRAS template sometime later, this will also change the abstract form.
       }
   }

% \abstract{}{}{}{}{} 
% 5 {} token are mandatory
 
  \abstract
  % context heading (optional)
  % {} leave it empty if necessary  
   {In recent years, thousands of exoplanets of been discovered. 
Given this large amount of data, it is imperative that we optimize our ability to perform analysis. 
One of the techniques who are used to discover or analyze exoplanets are radial velocities surveys. 
In order to fit this type data, Markov Chain Monte Carlo (MCMC) methods are often applied.}
  % aims heading (mandatory)
   {In this work, we determine the effectiveness of langevin MCMC methods on radial velocity fitting. 
This algorithm produces less correlated samples but each proposal is more costly. 
We aim investigate to if this leads to an increased overall efficiency or not.}
  % methods heading (mandatory)
   {The performance we obtain is compared with the performance of two other standard MCMC, the affine invariant sampler and HMC.
We do this using time normalized effective sample size as our measure for a few test problems. 
\memohr{What does this have to do with methods? It also contradicts the results below.}
\memorl{I now clarify that the method we use to see if SMALA is better or worse is by comparing the MCMC with time normalized ESS as our measure.}
}
  % results heading (mandatory)
   {We observe that the computational costs associated with the second derivatives is too high when the dimensionality of the problem becomes large.
This costs outweighs some of the benefits from uncorrelated steps. Additionally, in high dimensional cases we see that our implemented langevin method has similar or worse autocorrelation times compared to the affine invariant sampler. This is most likely due to the complexity of the phase space.
\memohr{Isn't this the same as the last sentence?}
\memorl{There's a subtle difference which is discussed at length in the later parts. Not only does the computational cost go up, the effective sample size/autocorrelation time is practically the same in addition to that... Addressed by mentioning the complexity of the phase space.}.
}
  % conclusions heading (optional), leave it empty if necessary 
   {}

   \keywords{MCMC --
                Radial Velocity --
                Langevin Based Methods
               }

   \maketitle
%
%________________________________________________________________
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
With thousands of exoplanets having already been discovered \cite{exoArchive}, many methods are used or combined to either refine our information or make yet newer discoveries. 
The two biggest contributors are transit measurements and radial velocity surveys. 
It may not always be possible to invest into more computing power to process this data. 
In order to process this large volume of data we need to use efficient Markov Chain Monte Carlo (MCMC) techniques. 
One route to speed up convergence of posteriors is to have an MCMC which makes better proposals. 
The idea is that by making these better proposals, we obtain uncorrelated samples, and thus require a smaller sample size to achieve the same effective sample size (ESS).

This is what Langevin MCMC methods do by using derivative information of the likelihood space. 
These types of techniques have never been implemented in an exoplanet context, and pose various technical challenges. 
These are discussed at length in section \ref{setup}. 
To test Langevin methods we require the derivative information, thus we need an analytical coordinate system described by \cite{Pl2009} such that these values are always defined. 
This calculation needs to be as efficient as possible, so we use an implementation of \texttt{Rebound}'s variational equations from \cite{Rein2016} to compute the $\chi^2$ likelihood derivative. 
Alongside this, we carefully selected the hyperparameters of the MCMC methods, as explained in section \ref{hyper}.

In this paper we compare our implementation with another MCMC by using the CPU time-normalized effective sample size (shortened to efficiency). 
We use 3 sample fitting problems to compare the algorithms, where we initialize the MCMC with best known, or true initial conditions. 
These have dimensions $N=1$, $N=4$, and $N=10$, ranging from the most simplistic case to using real data from a multi-planet system which is in resonance. 
These results are presented in section \ref{results}.

\memohr{This section reads really well!}
 
\section{Methods and Setup}\label{setup}
\subsection{MCMC Methods}
The Langevin method we chose in our implementation is the Simplified manifold Metropolis-Adjusted Langevin Algorithm (SMALA). 
The SMALA algorithm simplifies the general N-dimensional MALA algorithm by assuming the space is mostly flat and eliminating the terms in the proposal which account for the curvature of the space \citep{Girolami2011}.
\memohr{I'm not sure this is a correct description of SMALA. Clearly, we take into account the curvature of the likelihood space.}
\memorl{We look at the curvature of the likelihood function which lives inside a certain space but the curvature of the space it's in is not considered. Specifically, Girolami drops the Christoffel symbols which would be required for a generic Riemanian manifold, here we just assume the likelihood function lives in a Cartesian coordinate (uncurved coordinate for the space) so these terms are 0, hence `simplified'.}
\memodt{Is it too simplified to say that MALA generates proposals on the full non-euclidean manifold, while SMALA always operates on the flat space tangent to the manifold (I'm sure there's a correct technical word for that...tangent bundle?). If this is accurate, maybe one could also say that SMALA is a faster, local approximation to MALA? I also think there needs to be a 1-2 sentence description of what MALA does before you can introduce SMALA.}

With this simplification, the SMALA approach can be explained with a simple example. 
Given a Gaussian likelihood space, the Metropolis-Hastings algorithm will randomly wander through the distribution, favoring movement towards the maximum likelihood.
Rather than doing random proposals, we can make more effective proposals if we assume our likelihood function is approximately parabolic near the maximum. 
The SMALA method will use the first and second derivatives of the sampler to project a parabola and sample in close proximity of the maximum. 
\memohr{This sounds like we're just doing an interation of Newton's method. Need to mention that SMALA does not really jump to the maximum.}
\memorl{Addressed. We *sample* near the maximum.}
Depending on how good this parabolic approximation is, we can expect two benefits.
\begin{itemize}
\item Reaching the neighborhood of the maximum likelihood with fewer steps.
\item Highly uncorrelated samples since we make larger jumps.
\end{itemize}
These properties are shown very well in the sample problems of \cite{Girolami2011}. 
However, this benefit comes with the computational cost of calculating the gradient and Hessian of the likelihood function, which will scale in $\mathcal{O}(n^2)$, $n$ being the dimensionality of the problem. 
\memohr{define $n$}
\memorl{Addressed.}

We will compare the algorithm with \memodt{EMCEE \citep{Foreman-Mackey2013}, which is based on} the affine invariant sampling MCMC from \cite{Goodman2010} \memodt{I agree with Hanno that you need to cite the original paper, but it might make sense to also cite it, since it's what you use and what most people in astro will have heard of}\memohr{Note that that FM2013 paper is mainly an implementation of the affine invariant sample. The only thing they add is splitting the sampler into two pools which allows for parallelization among other things. Should cite the original paper here.} \memorl{Good point, addressed.} and an implementation of Hamiltonian Monte Carlo (abbreviated as HMC) \citep{Duane1987}.
\memodt{Once you decide on above, rest of paragraph needs editing. Don't you do your own implementation for both SMALA and HMC? Since you have separate paragraphs for SMALA, it might read better if you explain invariant sampling MCMC in a separate pagraph, HMC in another, and then a short transition sentence saying you'll compare them}
While we created our own python implementation of SMALA, the affine invariant MCMC has its own python package implementation known as EMCEE which we have used in our framework. 
This MCMC uses an ensemble of walkers to make linear proposals based on certain `moves'. 
In this work we use simple `stretch moves' which are described in full detail by \cite{Foreman-Mackey2013}. 
The other MCMC we compare with is the HMC which uses Hamiltonian mechanics to create new proposals. 
For a particular state an arbitrary velocity kick is given, drawn from a standard normal distribution. 
This will allow generating a new proposal based on how the state or `particle' will travel on the likelihood function as described by Hamiltonian mechanics. We implement a leapfrog method for integrating the state vector forward.

\subsection{Measures to Ensure Stability and Correctness}
Various steps were taken to maximize the numerical stability and usability of the MCMC for fitting exoplanets. Some were directly implemented into \texttt{Rebound}, while others were implemented at a higher level within our framework.

\subsubsection{\reb Side\memodt{Implementation?}}\label{analytical}
\memodt{I've added the \reb command Hanno uses in his papers for consistency. Should replace other instances with same command}
In the framework of traditional orbital elements certain derivatives are undefined. \memodt{SMALA relies on derivatives with respect to the fitting parameters. However, traditional orbital elements are fraught with coordinate singularities that will cause these derivatives to diverge}. For example, the derivative with respect to the argument of periapsis, $\omega$, is undefined when the eccentricity is zero. 
To avoid this we use the coordinates defined by \cite{Pl2009} which are trivially analytical since they use trigonometric functions, and are thus infinitely differentiable. \memodt{maybe 'which are analytical and thus infinitely differentiable.' It's not completely trivial given that later you say there's a singularity at inc=180. Moved that sentence up here:}
These coordinates come with a caveat, as all derivatives are undefined when considering a perfectly retrograde planet \memodt{The only exception is a coordinate singularity for perfectly retograde obits}. However in such a case, the coordinate system can be reoriented or the planet can be offset by an arbitrarily small inclination. \memodt{Is this true? Don't the derivatives diverge smoothly as they approach i=180? offsetting should just make the (possibly large) error finite. Otherwise couldn't you just the traditional orbital elements with their singularities and use the same technique? I would maybe leave at reorienting}
\memohr{I don't understand this paragraph}
\memorl{Fixed. All derivatives break in the retrograde case, but in such a scenario the system can be reoriented.}
In his paper, we find transformations which maps the traditional orbital elements unto an alternative set, \memodt{Do you mean 'we', or 'Pal maps the trad...} $a, e, i, \omega, \Omega, M \to a, h, k, i_x, i_y, \lambda$ where $h$ and $k$ are the Lagrangian orbital elements, and the inclination and longitude are decomposed into $i_x$, $i_y$ \memodt{I'm sure it's right, but even having heard a lot about h and k, I've never heard them referred to as Lagrangian orbital elements. Maybe '$h$ and $k$, and $i_x$ and $i_y$ decompose the eccentricity vector and orbital plane orientation into Cartesian components'}, and $\lambda$ gives the mean longitude. 
%The equations describing this transformation can be found in the original paper. 
\memohr{This is basically just the chain rule. I don't think this deserves a discussion here.}
\memorl{Cleaned up a bit. We're good here.}


\memodt{I find this paragraph a bit confusing. Maybe something like 'In order to calculate the gradient and Hessian of the likelihood function as required by SMALA, we need first and second-order derivatives of the star's radial velocity at each observation time, with respect to the planetary orbital parameters we wish to fit.  The first and second-order variational equations described and implemented by (Rein ref) generate the derivatives for the cartesian positions and velocities alongside the trajectories as a function of time, with very high numerical accuracy. In order to generate derivatives with respect to the more physically motivated Pal variables, we implemented ... (one sentence description of what you did. You did the right thing by not elaborating on this piece (which took a lot of time and pain!), but you want to mention that it's hard so people recognize it. I might list the total number of derivative functions that are required)}
We combine these coordinates with the variational equations of \texttt{Rebound} to calculate the likelihood gradient and Hessian. 
These allow us to integrate the derivatives along with the dynamical system with very high numerical accuracy. 
These variational equations are initialized such to provide the response of the system with respect to the velocity component of the primary object. 
For example, a first order variation we could get would be $\frac{dv_x}{dh}$, and for a second order we could obtain $\frac{d^2v_x}{dhda}$. 
Given every one of these combinations we can build the $\chi^2$ Hessian with respect to the radial velocity measurements.

For our implementation in \texttt{Rebound} \memodt{is this really in REBOUND, or in your MCMC code?}, we return a likelihood of zero if two planets come within two Hill radii \memodt{is this mutual Hill radii (do you add the masses of the two planets together to calculate it)}? of each other. 
{\memodt Such systems would be Hill unstable (cite Gladman 1993) and scatter on the timescale of planetary conjunctions. This prevents strong nonlinearities for such unphysical configurations from interfering with the scheme (something like that?).}
%We assign a likelihood of 0 since we assume that any planetary system which is observed today must necessarily be long-term stable. 
%Unfortunately it is not computationally feasible to test the long-term stability of every sample, so we settle for this criteria even if the close encounter would not fully destabilize the system.
\memohr{I think the last to parasgrpahs can be summarized as follows: "If two planets come within two Hill radii of each other, the system is considered unstable and a likelihood of 0 is returned.}
\memorl{Shortened.}

\subsubsection{Python Side \memodt{MCMC implementation}}
In the python front end we use a few tricks to improve the numerical stability. 
To handle very low likelihoods without numerical errors growing large, we work in log-likelihood. \memodt{This is standard--I would remove}
In addition to this, we avoid correlation bias between the semi-major axis and the mean longitude by making our integrations two-sided, centered on time $t=0$ \memohr{I don't think this is clear. Maybe reference Eric's paper where he used this}.
This correlation stems from the fact that a small change in period can allow `wiggle room' for a small change in phase, unless we allow this small perturbation to affect the data symmetrically.
Full details on this can be found in [...]. \memodt{do you have an intuitive and quick explanation for why this works?}
\memorl{Can't seem to locate the paper in which he employs this method..., I've checked the ones which mentioned RV in the title.. \href{http://astro.psu.edu/people/ebf11}{link to his homepage which has a link to his publications}}

\memodt{I believe you did the above for all MCMCs. I think the below should be explained under SMALA in the next section. }
In order to avoid instability during the inversion of the Hessian we make use a type of absolute value function on the eigenvalues. \memodt{At least mention that the scheme requires inverting the Hessian.}
This forces the eigenvalues to be non-negative and above a certain threshold which is determined by a hyperparameter.
This function, called softabs, is described in \cite{softabs}.
\memodt{Maybe 'We use the \texttt{softabs} function \citep{softabs}, which provides an analytic prescription for smoothly forcing the eigenvalues to be non-negative and above a threshold set by a single hyperparameter $\alpha$.' (why are negative eigenvalues bad?)}
\memohr{The 'soft absolute value of the eigenvalues' is not very clear. Try to be more precise.} 
\memorl{Addressed.}
This function gives the same behavior as an absolute value operator except near $0$ where it is smoothed out according to a scaling variable $\alpha$.
An eigenvalue of $0$ assume a value of $1/\alpha$ instead of $0$, which would be problematic, when we use this function. \memodt{This should come at the top to motivate using softabs}

\subsection{Hyperparameters}\label{hyper}
Each MCMC algorithm has a set of hyperparameters which must be determined \memodt{tuned} for optimal performance. Here we describe the general procedure for selecting the hyperparameters for each MCMC.

One of the greatest assets of the affine invariant MCMC is the lack of hyperparameters that need to be set by the user and the ease of selection. \memodt{as far as I can tell, don't all MCMCs you discuss require 2 hyperparameters?}
Only two major hyperparameters need to be chosen, these are the number of walkers in the ensemble and the `aggressiveness of the stretch moves' (denoted as $a$ in the emcee documentation). \memodt{use consistent notation. Earlier you used EMCEE}
\memohr{You first say that every MCMC has hyperparameters, then that the affine invarient sampler has none. Then finally you say that it has two! This can be simplified!}
\memohr{I'd be consistent and either call it affine invarient sapler or emcee. People might get lost if you switch back and forth.}

The performance of emcee increases with more walkers, for our test problems we have found 32 walkers to be sufficient as adding more did not give significant gains in performance. 
\memohr{Again, you can't say the opposite in two sentences that follow each other. Either the performance increases, or it does not increase. I know what you want to say, but it needs to be more precise and ideally more concise.}
The $a$ parameter was left at the recommended default value (a=2), we refer the reader to \cite{Foreman-Mackey2013} for a more detailed explanation on the significance of the parameter. 
\memohr{You start this section by saying that you describe how to select hyper parameters. But know you say you just stick to the previsouly used one! Again, I know what you've done and why you keep it fixed at 2, but you can't say the opposite to what you've said before}
\memohr{I'm sorry. By now this page looks like one of those PhD comics where the advisers gives back a draft to the student. It looks worse than it is! I'm pointing these things out because it is your first paper. Otherwise I would just correct it myself. But I want you to see what I would like to change and why.}
Another factor to consider is how the walkers are initialized. In general, initializing the ensemble as a small Gaussian sphere works remarkably well in almost every scenario. 
However, this approach may require a significant burn-in phase as the walkers expand to occupy the local shape of the likelihood space.

 
For SMALA we need to specify a step scale $\epsilon$ and the softabs' $\alpha$ hyperparameter. \memodt{bring $\alpha$ paragraph down to this section}
The $\epsilon$ sets the `aggressiveness' of proposals and is used to fine-tune the acceptance rate. 
Geometrically, this corresponds to how far up the N-dimensional parabola we let the added `randomness' go. 
According to \cite{robert1998} the optimal acceptance rate tends towards $57\%$ as the dimensionality goes to infinity, thus we fix \memodt{tune?} $\epsilon$ to give an acceptance rate in this neighborhood. 
The intuition for the $\alpha$ parameter is that it gives and maximum width to the proposal parabola \memodt{of scale 1/$\alpha$?.}
In the limiting case where this parameter is miscalculated \memodt{set too high} and inhibits the MCMC's ability to make proposals, SMALA will behave like a Metropolis-Hastings MCMC. 
This is shown in Figure \ref{alpha}. \memodt{is it really showing a case where $\alpha$ is set too high? The plot might more clearly show what alpha does if it had red points for a run with no softabs (or with an extreme alpha that only prevents it from crashing).} 

\memohr{In general the phrases 'works remarkably well' and 'in almost every scenario' are not very precise. Either say it works in all cases tested in this paper. Or way when it does not work. Having the information that it sometimes works and sometimes not, is not very useful by itself.}

For SMALA we need to specify a step scale $\epsilon$ and the softabs' $\alpha$ hyperparameter. \memodt{bring $\alpha$ paragraph down to this section}
The $\epsilon$ sets the `aggressiveness' of proposals and is used to fine-tune the acceptance rate. 
Geometrically, this corresponds to how far up the N-dimensional parabola we let the added `randomness' go. 
According to \cite{robert1998} the optimal acceptance rate tends towards $57\%$ as the dimensionality goes to infinity, thus we tune $\epsilon$ to give an acceptance rate in this limit.
The intuition for the $\alpha$ parameter is that it gives and maximum width to the proposal parabola. 
In the limiting case where this parameter is miscalculated \memodt{set too high} and inhibits the MCMC's ability to make proposals, SMALA will behave like a Metropolis-Hastings MCMC. 
This is shown in Figure \ref{alpha}. \memodt{is it really showing a case where $\alpha$ is set too high? The plot might more clearly show what alpha does if it had red points for a run with no softabs (or with an extreme alpha that only prevents it from crashing).} 
\memohr{You need to introduce Figure 1 a bit more. What is shown, what is the test case, etc, basically what you have in the figure caption. The captions need to be very short and concise. }

While if we let $\alpha$ be too large, this may lead to numerical instabilities. 
In the context of exoplanets, both of the aforementioned parameters should approximately be of order unity.
\memodt{why don't you have to choose an alpha for each parameter?}

\begin{figure}
\centering
\includegraphics[width=0.95\hsize]{alpha-1.png}
   \caption{This figure shows the effect of the alpha hyper-parameter on SMALA's sampling by considering 3 cases with increasing errors. 
\memodt{We simulate a one-planet system and superimpose gaussian noise at three scales...We then fix all orbital parameters at their true values and fit only for the mass. In this simple problem we expect...}
This system contains one planet with only mass as a parameter, hence in this simple problem we expect the likelihood space to follow the parabolic approximation very closely. 
This is seen in cases one and two of this example. 
Increasing the error on the data affects the sensitivity of the derivatives.
In case 3 where the error is greatest, the $\alpha$ parameter takes effect and limits the proposals. 
All three parabolas are generated from the Hessian evaluated at the true mass of the system.}
      \label{alpha}
\end{figure}

When using HMC we have three major hyperparameters to select. 
We must pick out a $\delta$ for the leapfrog steps, the number of leapfrog steps $L$ before testing a proposal state, and we need an appropriate mass matrix $M$ which corresponds the the scales and correlation of the parameters. 
A complete description can be found in chapter 5 of the MCMC Handbook \cite{1206.1901}. 
Due to the cost of evaluating the likelihood and its derivatives, $L=1$ gives the best efficiency score even when $N=1$. 
\memohr{Please state if you found this out or if this comes from some other work.}
The $\delta$ parameter is degenerate in the sense that changing it corresponds to the same effect as multiplying the mass matrix by some coefficient.
Hence, we chose a $\delta$ of order unity. \memodt{If this is true, then there are only two hyperparameters and you might not even mention delta}
For the selection of the matrix $M$ we use have opted to only specify the diagonal as doing all $N^2$ elements would be very time consuming.
The optimal acceptance rate for the HMC algorithm is determined by \cite{1001.4460}. 
In the limiting case of $L=1$ we obtain best results near an acceptance rate of $57\%$, in contrast with the optimal results for $L>1$ which are a few percent higher at $65\%$. 
\memohr{I'm confused. You just said L=1 has the best acceptance ratio?}
In addition to having an appropriate acceptance rate, we require the diagonal of $M$ to give autocorrelation rates which are approximately equal.
A good initial guess of the diagonal can be given by the inverse of the parameters. 
These masses are then tweaked according to the guidelines above.

\memodt{If it's really the case that HMC has an NxN mass matrix and L to pick, and the others really only have two scalar hyperparameters, then it's misleading to say they all have 2 hyperparameters. Of course you don't see that for this 1-D example, but I think you want to state the number of scalar hyperparameters each one has (I guess HMC would be NxN + 1), to really motivate why certain ones are easier for the user.}
\memohr{Again, this is just a bit too vague. What does tweaking mean here? And how about parameters such as angles? Or parameters which are initially set to 0 like the eccentricirty?}

\memodt{I think the rest of this section might make more sense in Sec. 3, since you keep having to refer forward to it. Sec. 3 might start with a subsection on metrics that defines ESS, and then start with this discussion of tuning hyperparameters in your N=1 example.}
Consider an example of $N=4$ \memodt{you used $n$ before}. 
Given the above procedure guidelines, say we would obtain an acceptance rate of $57\%$ with autocorrelations $AC_N = 12, 5, 2, 50$ with a first guess.
This would not constitute an appropriate MCMC run as one of the parameters will be greatly under determined due to inefficient sampling in that dimension. 
In contrast, if we would change the mass hyperparameters to give an acceptance rate of $54\%$ with $AC_N = 6, 7, 5, 8$, this would result in an appropriate instance of the MCMC with greater efficiency than the former. 
Namely, the efficiency is limited by the longest autocorrelation time (which gives the smallest effective sample size). 
\memohr{You define ESS later, but already use it here.}
Unfortunately, a certain amount of trial and error, and thus human labor, is required in order to find appropriate hyperparameters. 
One of the advantages of EMCEE and SMALA over HMC is the ease of selecting these hyperparameters. \memodt{So is it true that you have to tune hyperparameters not only to get a good acceptance rate, but to get a short and uniform autocorrelation time in each of the parameters? Is this not true for EMCEE and SMALA?}

Next we consider a short analysis of the local sensitivity of the hyperparameters for HMC and SMALA. 
This provides a rough idea to what order of magnitude \memodt{precision} the hyperparameters must be selected in practice. 
To quantify this we use a Monte Carlo approach and do 128 MCMC runs of an $N=4$ system of two planets (see Section \ref{n4section}) for both MCMC.
With each run a chosen hyperparameter is randomized. The measure of efficiency is explained in Section \ref{results}, however the most noteworthy point is that this score is a function of random variables. 
This measure will converge in the limit of infinite MCMC steps. 
For this Monte Carlo data, each run will have approximately 1000 autocorrelation times (as measured from near optimal performance) or alternatively, a few hundred effective samples. 
This corresponds to approximately 10000 steps for each SMALA run and 25000 steps for each HMC score evaluation. 
\memohr{I'm not sure what the last few sentences are about.}
\memohr{It's also a bit confusing to refer to section 3 and 3.2 here. Am I supposed to read section 3 now?}

In Figure \ref{sensfig} one can see the efficiency score as a function of hyperparameter value. 
In the upper portion of the plot we see SMALA, while the lower subplot shows the HMC performances. For SMALA we see performance smoothly decreasing as the hyperparameters receives larger perturbations from the optimal. 
However, we note that there are a few points near optimal $\epsilon$ which give low scores. 
These correspond to MCMC runs where the algorithm becomes entrapped in a part of phase space. 
These are rare occurrences when the MCMC fails to make good proposals for many steps in a row. \memodt{If true, I think you want to emphasize that SMALA gets out of these eventually, but getting stuck in them lowers the efficiency}
For the HMC runs we observe that the hyperparameter is less smooth and appears to mimic a stepwise function on the left of the plot. 
This qualitatively shows that we may obtain non-trivial behavior when trying to optimize the HMC hyperparameters. 
As such, it can be somewhat challenging for a human \memohr{I guess it's challenging, period.} to find the best mass hyperparameters for HMC in high dimensional cases given increasing sensitivity. 
This is expected as the likelihood space becomes more complex.

\begin{figure}
\centering
\includegraphics[width=0.95\hsize]{sensitivity-1.png}
   \caption{This figure shows the hyperparameter sensitivity of SMALA and HMC for a 4 dimensional parameter space. 
We observe a few rare events where SMALA fails to perform optimally and see the HMC score exhibit non-trivial behavior in the left portion of the lower plot. 
The width of the bands give an estimation of the error for a few hundred effective samples.}
      \label{sensfig}
\end{figure}

\section{Results}\label{results}
\subsection{Simple Test, $N=1$}
We use this simple test case to verify the algorithms and introduce the time-normalized ESS which is \memodt{often} used to compare the effectiveness of \memodt{different MCMCs \citep[e.g.,][]{Girolami2011, 1504.01418, Meyer2016, Lan2015}}.
%This method is used in other comparative works such as \cite{Girolami2011, 1504.01418, Meyer2016, Lan2015} and many others. 
The total ESS is limited by the parameter which \memodt{that} has the greatest autocorrelation time, $$ESS = \min_N\bigg( \frac{N}{(1+2\sum AC_k)}\bigg)$$. \memodt{One can also compare the ESS normalized by the total CPU time in seconds as a measure of computational efficiency}. 
\memodt{above $AC_k$ would be the autocorrelation time for a parameter. Here it is the value at different points in the chain? Need to define and put sum over k}
For this $N=1$ case we consider a planet with semi-major axis of $0.320$ and about $2.05$ Jupiter masses \memodt{what about the rest of the parameters? circular? inclined?}.% observed for a few orbits.
The generated data has \memodt{gaussian?} errors of $10$ \si{\metre\per\second} \memodt{that in turn have a small (specify?) variance in their standard deviations?}with a small variance and was integrated for three and a half orbits.
The fitting parameter selected is the semi-major axis.

The initial conditions of the MCMC are the same as the true conditions. 
The resulting radial velocity curve is shown in Figure \ref{FigSimple}. 
In the upper part of the figure we see the initial conditions \memodt{true solution in black?} overlapped with \memodt{green} traces of accepted proposals \memodt{, with data points and errors in red}. 
\memodt{In the middle panel}Below we find the curve generated from taking the average of the resulting posterior samples, \memodt{and in the bottom panel the residuals between the fit in the middle panel and the data}along with residuals.

\begin{figure}
\centering
\includegraphics[width=0.95\hsize]{rv1-1.png}
   \caption{This figure shows the results for the simple test case with SMALA. 
In the top panel we find the RV curve of the initial conditions in purple and 50 random trails generated from samples in the posterior. 
This shows the spread in how well the samples fit the curve. 
The middle and bottom subplot shows the average result after the burn in phase and the residuals of that fit, we can see that the initial conditions are well recovered.}
      \label{FigSimple}
\end{figure}

In this simple case 1-D case the SMALA algorithm strongly outperforms EMCEE and moderately outperforms HMC. 
This is clearly shown in the first row of Table \ref{Table1}.
This is due to the low cost of calculating the first and second derivatives in this sample 1-D problem. 
The highly uncorrelated samples lead to a large ESS, this combined with the above makes the SMALA MCMC very efficient in the one dimension case, also surpassing HMC. All algorithms perfectly \memodt{remove perfectly} recover the original conditions and give identical \memodt{statistically indistinguishable} distributions.

\memodt{Table 1 is a bit confusing. You say SMALA strongly outperforms EMCEE, but EMCEE had a shorter runtime. Does that mean that EMCEE was not run long enough to get an accurate distribution? Is there not a good stopping condition?}

\begin{table}
\caption{MCMC Efficiency Results}             % title of Table
\label{Table1}      % is used to refer this table in the text
\centering                         
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c c c c}     
\hline\hline               
 & MCMC & Total Iterations & Minimum ESS & Total CPU-Time ($s$) & ESS/T \\    % table heading 
\hline                     
   \multirow{3}{*}{$N=1$} & EMCEE & 768000 & 25560 & 5783 & 4.42 \\ 
   & HMC & 320000 & 41004 & 4592 & 8.92 \\ 
   & SMALA& 320000 & 213222 & 7554 & 28.2 \\
\hline                                  
   \multirow{3}{*}{$N=4$} & EMCEE & 768000 & 14076 & 25233 & 0.558 \\      
   & HMC & 320000 & 16466 & 45827 & 0.360 \\ 
   & SMALA& 320000 & 58688 & 172170 & 0.341 \\
\hline                                  
   \multirow{3}{*}{$N=10$} & EMCEE & 768000 & 1325 & 39292 & 0.03372 \\      
   & HMC & 320000 & 406 & 123940 & 0.00272 \\ 
   & SMALA& 320000 & 241 & 1086615 & 0.00022 \\
\hline                                   
\end{tabular}
}
\end{table}

\subsection{2-Planet Case, $N=4$}\label{n4section}

In this problem we try fitting four parameters with two planets.
\memodt{For each orbit, we consider its semi-major axis and mean longitude.}
As with the previous example, we start from the true parameters and run both MCMCs for a few hundred thousand steps. 
The synthetic observations have 250 data points with errors centered on $10$ \si{\metre\per\second}. 
The data points are spread out over about a dozen orbits.

The resulting time normalized ESS are $0.558$, $0.360$, and $0.341$ for EMCEE, HMC, and SMALA respectively, as noted in Table \ref{Table1}. 
For four parameters, the SMALA algorithm will require a much longer runtime than EMCEE but will still generate posterior samples which are uncorrelated enough to compensate. 
The HMC algorithm shows the same behavior but with less intensity. For this case, the results of the algorithms quite similar and show approximately the same efficiency.


\subsection{Real Data: HD155358, $N=10$}

Now we consider a real data example using the RV data available for HD155358 from \cite{Robertson2012}. 
In this fit we solve for for ten parameters. For each orbit we \memodt{fix the line of nodes} and fit for $a$, $m\, \sin i$\footnote{Since the $i_x$, $i_y$ parameters are excluded, we are effectively fitting the degenerate mass.\memodt{I thought you're fitting for sin i, isn't the only thing your'e not fitting for $\Omega$?}}, $h$, $k$ and $\lambda$ as described in Section \ref{analytical}. 
The parameters obtained fall within errors of the original work by \cite{Robertson2012}.

In this scenario the EMCEE algorithm performs much better than SMALA and HMC, having a much higher efficiency by orders of magnitude. 
This is shown in the last row of Table \ref{Table1} with EMCEE giving $0.03372$ effective samples per second compared to HMC's $0.00272$ and SMALA's $0.00022$.

\begin{figure}
\centering
\includegraphics[width=0.95\hsize]{rv3-1.png}
   \caption{This figure shows the results for the HD155358 system. 
The uppermost plot shows the initial conditions of the system and the spread in the radial velocity curves generated from the posterior. 
The initial conditions used were provided by Robertson \cite{Robertson2012}. 
The radial velocity curve of the average parameters and residuals are presented below. 
The resulting parameters can be found in the appendix.}
      \label{FigHD}
\end{figure}

\section{Discussion \& Conclusion}
In general, when dealing with a full multi-planet system the SMALA algorithm does not perform well. 
Even in the smallest possible real case of $N=7$ parameters (mass \& 6 orbital parameters) SMALA will be outperformed by EMCEE \& HMC. 
The computational cost of calculating the metric is no longer beneficial at this dimensionality. 
When comparing the effective sample sizes, SMALA only produced less correlated samples in the cases of $N \leq 4$ parameters, while emcee performs better for the $N>4$ cases. 
We suspect that in addition to the computational cost, the complexity and nonlinearity of the high dimensional likelihood space limits the effectiveness of the MCMC, as it becomes very challenging to produce highly uncorrelated samples at a reasonable cost. 
To support this claim on nonlinearity, when we consider a system with two planets that strongly interact, we can observe these effects. 
This appears in some of the posterior samples, as shown by the trails in Figure \ref{FigHD}, we see these strong nonlinear effects on the left side as some RV trails extend above the $0.002$ mark. 
When the likelihood space exhibits such behavior, we might not expect the parabolic approximation to perform so well. 
As we get many samples in the phase space neighborhood of collisions, the likelihood manifold becomes curved and distorted. 
This effect may create a minimum autocorrelation threshold which is reached by the SMALA MCMC. 
One possible solution to this dilemma would be to acquire data with smaller errors, this geometrically corresponds to tightening the likelihood well, as shown in Figure \ref{alpha}. \memodt{This is spoken like a true theorist! Unfortunately in practice, you start with a fixed dataset, and then want to fit it.}
This would make the space near the maximum adhere more closely to a parabolic approximation, and prevent the MCMC from wandering in the regions of phase space where collisions and nonlinear effects are likely. 
However, even with advances in RV instrumentation, it unlikely feasible to obtain errors small enough to resolve these issues via sharper, more `parabola-shaped' likelihoods. 
Even if we do obtain such small errors, the computational cost will most likely dominate and EMCEE will be the most efficient MCMC.

To summarize, the costs of generating an accurate Hessians outweighs the benefits of highly uncorrelated steps at high dimensionality. 
The EMCEE algorithm will outperform SMALA by orders of magnitude in large multi-planet systems encountered in practice. 
The dimensionality threshold where the affine invariant MCMC and the SMALA MCMC perform comparably is around $N = 4$, meaning that even for a single planet system EMCEE or HMC will be more efficient.
\memodt{Have you talked with Ben Nelson and/or Eric Ford about this? When I talked with Ben at Aspen, he thought exactly the opposite--that EMCEE is hopeless in high dimensional systems, and that SMALA should help. He might have some ideas?}

In future works we will consider hybrid MCMC which use combinations of  EMCEE, HMC, and SMALA steps to achieve maximal efficiency by giving each MCMC specific combinations parameter. 
\memodt{At some point it should be introduced in the paper that you are always starting from the true solution, where local approximations are going to do well, whereas you don't ahve that luxury in practice. I'm not sure whether or not that's related to Gibbs sampling or not...}
Another example for potential improvements would be to consider Gibbs-SMALA or maybe planet-wise blocked Gibbs sampler approach which could speed up the algorithm. 
An additional pursuit to improve the speed of analysis might be to use the analytical solution to Kepler's equations in our proposals instead of variational equations, this would be much more computationally cheap. 
A consequence of this approach would be the introduction of errors in the derivative, as N-body effects would not be considered. \memodt{I would phrase this as a separate direction. You're doing N-body fitting. Many others before have done Keplerian fits--you could simply say that this method might do well in that case where the derivatives are analytic}
Another modification to consider would be the ALSMALA algorithm which uses partial metric updates \cite{1608.07986}. 
Using an inaccurate metric will lead to more correlated samples, however there may exist an optimum point where overall performance is enhanced.

\begin{acknowledgements}
	This research was funded by NSERC Discovery Grant RGPIN-2014-04553. 
Computations were performed on the GPC supercomputer at the SciNet HPC Consortium. 
SciNet is funded by: the Canada Foundation for Innovation under the auspices of Compute Canada; the Government of Ontario; Ontario Research Fund - Research Excellence; and the University of Toronto.
\end{acknowledgements}


%-------------------------------------------------------------------
%\pagebreak

\bibliographystyle{apa}
\bibliography{Bibliography}

\begin{appendix} 

\begin{sidewaystable*}
\caption{MCMC Average Resulting Parameters with 95\% Credible Intervals}\label{Table4}
\centering
\begin{tabular}{c c c c c c c c c c c c}        % centered columns (4 columns)
\hline\hline                 % inserts double horizontal lines
 & MCMC & $a_0$ & $h_0$ & $k_0$ & $m_0$ ($10^{-4}$) & $\lambda_0$ & $a_1$ & $h_1$ & $k_1$ & $m_1$ ($10^{-4}$) & $\lambda_1$ \\    % table heading 
\hline                        % inserts single horizontal line
   \rule{0pt}{4ex}  \multirow{5}{*}{$N=1$} & EMCEE & $0.3192^{+0.0071}_{-0.0076}$ & - & - & - & - & - & - & - & - & - \\
   \rule{0pt}{4ex} & SMALA &  $0.3193^{+0.0071}_{-0.0076}$ & - & - & - & - & - & - & - & - & -\\
    \rule{0pt}{4ex} & HMC &  $0.3194^{+0.0048}_{-0.0051}$ & - & - & - & - & - & - & - & - & -\\
   \rule{0pt}{4ex}  \multirow{5}{*}{$N=4$} & EMCEE & $0.3198^{+0.0026}_{-0.0026}$ & - & - & - & $1.711^{+0.187}_{-0.184}$ & $0.5817^{+0.0244}_{-0.0221}$ & - & - & - & $1.031^{+0.396}_{-0.394}$ \\
   \rule{0pt}{4ex} & SMALA &  $0.3199^{+0.0026}_{-0.0026}$ & - & - & - & $1.712^{+0.186}_{-0.184}$ & $0.5817^{+0.0244}_{-0.0221}$ & - & - & - & $1.030^{+0.390}_{-0.388}$\\
    \rule{0pt}{4ex} & HMC &  $0.3199^{+0.0019}_{-0.0019}$ & - & - & - & $1.711^{+0.087}_{-00186}$ & $0.5808^{+0.0156}_{-0.0149}$ & - & - & - & $1.035^{+0.152}_{-0.159}$\\
   %\rule{0pt}{3ex}
   \rule{0pt}{4ex} \multirow{5}{*}{$N=10$} & EMCEE & $0.6586^{+0.0087}_{-0.0089}$ & $-0.11^{+0.35}_{-0.31}$ & $-0.12^{+0.31}_{-0.32}$ & $8.1^{+3.3}_{-3.4}$ & $4.52^{+0.55}_{-0.50}$ & $1.046^{+0.024}_{-0.026}$ & $-0.14^{+0.52}_{-0.45}$ & $-0.05^{+0.50}_{-0.48}$ & $8.5^{+3.5}_{-3.2}$ & $1.54^{+0.62}_{-0.58}$ \\
   \rule{0pt}{4ex} & SMALA & $0.6508^{+0.0077}_{-0.0076}$ & $-0.14^{+0.31}_{-0.30}$ & $-0.10^{+0.30}_{-0.27}$ & $8.2^{+3.4}_{-3.0}$ & $4.54^{+0.54}_{-0.52}$ & $1.046^{+0.022}_{-0.026}$ & $-0.13^{+0.52}_{-0.42}$ & $-0.04^{+0.47}_{-0.46}$ & $8.6^{+3.5}_{-3.2}$ & $1.52^{+0.60}_{-0.55}$\\
   \rule{0pt}{4ex} & HMC & $0.6583^{+0.0074}_{-0.0067}$ & $-0.14^{+0.22}_{-0.19}$ & $-0.10^{+0.16}_{-0.15}$ & $8.4^{+3.0}_{-2.6}$ & $4.49^{+0.41}_{-1.46}$ & $1.046^{+0.020}_{-0.024}$ & $-0.12^{+0.39}_{-0.36}$ & $-0.04^{+0.36}_{-0.64}$ & $8.6^{+2.6}_{-2.4}$ & $1.51^{+0.44}_{-0.37}$\\
   %\rule{0pt}{3ex}
\hline  
\end{tabular}
\end{sidewaystable*}

\end{appendix}

%\pagebreak
%\newpage

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Examples for figures using graphicx
A guide "Using Imported Graphics in LaTeX2e"  (Keith Reckdahl)
is available on a lot of LaTeX public servers or ctan mirrors.
The file is : epslatex.pdf 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%_____________________________________________________________
%                 A figure as large as the width of the column
%-------------------------------------------------------------
   \begin{figure}
   \centering
   \includegraphics[width=\hsize]{empty.eps}
      \caption{Vibrational stability equation of state
               $S_{\mathrm{vib}}(\lg e, \lg \rho)$.
               $>0$ means vibrational stability.
              }
         \label{FigVibStab}
   \end{figure}
%
%_____________________________________________________________
%                                    One column rotated figure
%-------------------------------------------------------------
   \begin{figure}
   \centering
   \includegraphics[angle=-90,width=3cm]{empty.eps}
      \caption{Vibrational stability equation of state
               $S_{\mathrm{vib}}(\lg e, \lg \rho)$.
               $>0$ means vibrational stability.
              }
         \label{FigVibStab}
   \end{figure}
%
%_____________________________________________________________
%                        Figure with caption on the right side 
%-------------------------------------------------------------
   \begin{figure}
   \sidecaption
   \includegraphics[width=3cm]{empty.eps}
      \caption{Vibrational stability equation of state
               $S_{\mathrm{vib}}(\lg e, \lg \rho)$.
               $>0$ means vibrational stability.
              }
         \label{FigVibStab}
   \end{figure}
%
%_____________________________________________________________
%
%_____________________________________________________________
%                                Figure with a new BoundingBox 
%-------------------------------------------------------------
   \begin{figure}
   \centering
   \includegraphics[bb=10 20 100 300,width=3cm,clip]{empty.eps}
      \caption{Vibrational stability equation of state
               $S_{\mathrm{vib}}(\lg e, \lg \rho)$.
               $>0$ means vibrational stability.
              }
         \label{FigVibStab}
   \end{figure}
%
%_____________________________________________________________
%
%_____________________________________________________________
%                                      The "resizebox" command 
%-------------------------------------------------------------
   \begin{figure}
   \resizebox{\hsize}{!}
            {\includegraphics[bb=10 20 100 300,clip]{empty.eps}
      \caption{Vibrational stability equation of state
               $S_{\mathrm{vib}}(\lg e, \lg \rho)$.
               $>0$ means vibrational stability.
              }
         \label{FigVibStab}
   \end{figure}
%
%______________________________________________________________
%
%_____________________________________________________________
%                                             Two column Figure 
%-------------------------------------------------------------
   \begin{figure*}
   \resizebox{\hsize}{!}
            {\includegraphics[bb=10 20 100 300,clip]{empty.eps}
      \caption{Vibrational stability equation of state
               $S_{\mathrm{vib}}(\lg e, \lg \rho)$.
               $>0$ means vibrational stability.
              }
         \label{FigVibStab}
   \end{figure*}
%
%______________________________________________________________
%
%_____________________________________________________________
%                                             Simple A&A Table
%_____________________________________________________________
%
\begin{table}
\caption{Nonlinear Model Results}             % title of Table
\label{table:1}      % is used to refer this table in the text
\centering                          % used for centering table
\begin{tabular}{c c c c}        % centered columns (4 columns)
\hline\hline                 % inserts double horizontal lines
HJD & $E$ & Method\#2 & Method\#3 \\    % table heading 
\hline                        % inserts single horizontal line
   1 & 50 & $-837$ & 970 \\      % inserting body of the table
   2 & 47 & 877    & 230 \\
   3 & 31 & 25     & 415 \\
   4 & 35 & 144    & 2356 \\
   5 & 45 & 300    & 556 \\ 
\hline                                   %inserts single line
\end{tabular}
\end{table}
%
%_____________________________________________________________
%                                             Two column Table 
%_____________________________________________________________
%
\begin{table*}
\caption{Nonlinear Model Results}             
\label{table:1}      
\centering          
\begin{tabular}{c c c c l l l }     % 7 columns 
\hline\hline       
                      % To combine 4 columns into a single one 
HJD & $E$ & Method\#2 & \multicolumn{4}{c}{Method\#3}\\ 
\hline                    
   1 & 50 & $-837$ & 970 & 65 & 67 & 78\\  
   2 & 47 & 877    & 230 & 567& 55 & 78\\
   3 & 31 & 25     & 415 & 567& 55 & 78\\
   4 & 35 & 144    & 2356& 567& 55 & 78 \\
   5 & 45 & 300    & 556 & 567& 55 & 78\\
\hline                  
\end{tabular}
\end{table*}
%
%-------------------------------------------------------------
%                                          Table with notes 
%-------------------------------------------------------------
%
% A single note
\begin{table}
\caption{\label{t7}Spectral types and photometry for stars in the
  region.}
\centering
\begin{tabular}{lccc}
\hline\hline
Star&Spectral type&RA(J2000)&Dec(J2000)\\
\hline
69           &B1\,V     &09 15 54.046 & $-$50 00 26.67\\
49           &B0.7\,V   &*09 15 54.570& $-$50 00 03.90\\
LS~1267~(86) &O8\,V     &09 15 52.787&11.07\\
24.6         &7.58      &1.37 &0.20\\
\hline
LS~1262      &B0\,V     &09 15 05.17&11.17\\
MO 2-119     &B0.5\,V   &09 15 33.7 &11.74\\
LS~1269      &O8.5\,V   &09 15 56.60&10.85\\
\hline
\end{tabular}
\tablefoot{The top panel shows likely members of Pismis~11. The second
panel contains likely members of Alicante~5. The bottom panel
displays stars outside the clusters.}
\end{table}
%
% More notes
%
\begin{table}
\caption{\label{t7}Spectral types and photometry for stars in the
  region.}
\centering
\begin{tabular}{lccc}
\hline\hline
Star&Spectral type&RA(J2000)&Dec(J2000)\\
\hline
69           &B1\,V     &09 15 54.046 & $-$50 00 26.67\\
49           &B0.7\,V   &*09 15 54.570& $-$50 00 03.90\\
LS~1267~(86) &O8\,V     &09 15 52.787&11.07\tablefootmark{a}\\
24.6         &7.58\tablefootmark{1}&1.37\tablefootmark{a}   &0.20\tablefootmark{a}\\
\hline
LS~1262      &B0\,V     &09 15 05.17&11.17\tablefootmark{b}\\
MO 2-119     &B0.5\,V   &09 15 33.7 &11.74\tablefootmark{c}\\
LS~1269      &O8.5\,V   &09 15 56.60&10.85\tablefootmark{d}\\
\hline
\end{tabular}
\tablefoot{The top panel shows likely members of Pismis~11. The second
panel contains likely members of Alicante~5. The bottom panel
displays stars outside the clusters.\\
\tablefoottext{a}{Photometry for MF13, LS~1267 and HD~80077 from
Dupont et al.}
\tablefoottext{b}{Photometry for LS~1262, LS~1269 from
Durand et al.}
\tablefoottext{c}{Photometry for MO2-119 from
Mathieu et al.}
}
\end{table}
%
%-------------------------------------------------------------
%                                       Table with references 
%-------------------------------------------------------------
%
\begin{table*}[h]
 \caption[]{\label{nearbylistaa2}List of nearby SNe used in this work.}
\begin{tabular}{lccc}
 \hline \hline
  SN name &
  Epoch &
 Bands &
  References \\
 &
  (with respect to $B$ maximum) &
 &
 \\ \hline
1981B   & 0 & {\it UBV} & 1\\
1986G   &  $-$3, $-$1, 0, 1, 2 & {\it BV}  & 2\\
1989B   & $-$5, $-$1, 0, 3, 5 & {\it UBVRI}  & 3, 4\\
1990N   & 2, 7 & {\it UBVRI}  & 5\\
1991M   & 3 & {\it VRI}  & 6\\
\hline
\noalign{\smallskip}
\multicolumn{4}{c}{ SNe 91bg-like} \\
\noalign{\smallskip}
\hline
1991bg   & 1, 2 & {\it BVRI}  & 7\\
1999by   & $-$5, $-$4, $-$3, 3, 4, 5 & {\it UBVRI}  & 8\\
\hline
\noalign{\smallskip}
\multicolumn{4}{c}{ SNe 91T-like} \\
\noalign{\smallskip}
\hline
1991T   & $-$3, 0 & {\it UBVRI}  &  9, 10\\
2000cx  & $-$3, $-$2, 0, 1, 5 & {\it UBVRI}  & 11\\ %
\hline
\end{tabular}
\tablebib{(1)~\citet{branch83};
(2) \citet{phillips87}; (3) \citet{barbon90}; (4) \citet{wells94};
(5) \citet{mazzali93}; (6) \citet{gomez98}; (7) \citet{kirshner93};
(8) \citet{patat96}; (9) \citet{salvo01}; (10) \citet{branch03};
(11) \citet{jha99}.
}
\end{table}
%_____________________________________________________________
%                      A rotated Two column Table in landscape  
%-------------------------------------------------------------
\begin{sidewaystable*}
\caption{Summary for ISOCAM sources with mid-IR excess 
(YSO candidates).}\label{YSOtable}
\centering
\begin{tabular}{crrlcl} 
\hline\hline             
ISO-L1551 & $F_{6.7}$~[mJy] & $\alpha_{6.7-14.3}$ 
& YSO type$^{d}$ & Status & Comments\\
\hline
  \multicolumn{6}{c}{\it New YSO candidates}\\ % To combine 6 columns into a single one
\hline
  1 & 1.56 $\pm$ 0.47 & --    & Class II$^{c}$ & New & Mid\\
  2 & 0.79:           & 0.97: & Class II ?     & New & \\
  3 & 4.95 $\pm$ 0.68 & 3.18  & Class II / III & New & \\
  5 & 1.44 $\pm$ 0.33 & 1.88  & Class II       & New & \\
\hline
  \multicolumn{6}{c}{\it Previously known YSOs} \\
\hline
  61 & 0.89 $\pm$ 0.58 & 1.77 & Class I & \object{HH 30} & Circumstellar disk\\
  96 & 38.34 $\pm$ 0.71 & 37.5& Class II& MHO 5          & Spectral type\\
\hline
\end{tabular}
\end{sidewaystable*}
%_____________________________________________________________
%                      A rotated One column Table in landscape  
%-------------------------------------------------------------
\begin{sidewaystable}
\caption{Summary for ISOCAM sources with mid-IR excess 
(YSO candidates).}\label{YSOtable}
\centering
\begin{tabular}{crrlcl} 
\hline\hline             
ISO-L1551 & $F_{6.7}$~[mJy] & $\alpha_{6.7-14.3}$ 
& YSO type$^{d}$ & Status & Comments\\
\hline
  \multicolumn{6}{c}{\it New YSO candidates}\\ % To combine 6 columns into a single one
\hline
  1 & 1.56 $\pm$ 0.47 & --    & Class II$^{c}$ & New & Mid\\
  2 & 0.79:           & 0.97: & Class II ?     & New & \\
  3 & 4.95 $\pm$ 0.68 & 3.18  & Class II / III & New & \\
  5 & 1.44 $\pm$ 0.33 & 1.88  & Class II       & New & \\
\hline
  \multicolumn{6}{c}{\it Previously known YSOs} \\
\hline
  61 & 0.89 $\pm$ 0.58 & 1.77 & Class I & \object{HH 30} & Circumstellar disk\\
  96 & 38.34 $\pm$ 0.71 & 37.5& Class II& MHO 5          & Spectral type\\
\hline
\end{tabular}
\end{sidewaystable}
%
%_____________________________________________________________
%                              Table longer than a single page  
%-------------------------------------------------------------
% All long tables will be placed automatically at the end, after 
%                                        \end{thebibliography}
%
\begin{longtab}
\begin{longtable}{lllrrr}
\caption{\label{kstars} Sample stars with absolute magnitude}\\
\hline\hline
Catalogue& $M_{V}$ & Spectral & Distance & Mode & Count Rate \\
\hline
\endfirsthead
\caption{continued.}\\
\hline\hline
Catalogue& $M_{V}$ & Spectral & Distance & Mode & Count Rate \\
\hline
\endhead
\hline
\endfoot
%%
Gl 33    & 6.37 & K2 V & 7.46 & S & 0.043170\\
Gl 66AB  & 6.26 & K2 V & 8.15 & S & 0.260478\\
Gl 68    & 5.87 & K1 V & 7.47 & P & 0.026610\\
         &      &      &      & H & 0.008686\\
Gl 86 
\footnote{Source not included in the HRI catalog. See Sect.~5.4.2 for details.}
         & 5.92 & K0 V & 10.91& S & 0.058230\\
\end{longtable}
\end{longtab}
%
%_____________________________________________________________
%                              Table longer than a single page
%                                             and in landscape 
%  In the preamble, use:       \usepackage{lscape}
%-------------------------------------------------------------
% All long tables will be placed automatically at the end, after
%                                        \end{thebibliography}
%
\begin{longtab}
\begin{landscape}
\begin{longtable}{lllrrr}
\caption{\label{kstars} Sample stars with absolute magnitude}\\
\hline\hline
Catalogue& $M_{V}$ & Spectral & Distance & Mode & Count Rate \\
\hline
\endfirsthead
\caption{continued.}\\
\hline\hline
Catalogue& $M_{V}$ & Spectral & Distance & Mode & Count Rate \\
\hline
\endhead
\hline
\endfoot
%%
Gl 33    & 6.37 & K2 V & 7.46 & S & 0.043170\\
Gl 66AB  & 6.26 & K2 V & 8.15 & S & 0.260478\\
Gl 68    & 5.87 & K1 V & 7.47 & P & 0.026610\\
         &      &      &      & H & 0.008686\\
Gl 86
\footnote{Source not included in the HRI catalog. See Sect.~5.4.2 for details.}
         & 5.92 & K0 V & 10.91& S & 0.058230\\
\end{longtable}
\end{landscape}
\end{longtab}
%
% Online Material
%_____________________________________________________________
%        Online appendices have to be placed at the end, after
%                                        \end{thebibliography}
%-------------------------------------------------------------
\end{thebibliography}

\Online

\begin{appendix} %First online appendix
\section{Background galaxy number counts and shear noise-levels}
Because the optical images used in this analysis...

\begin{figure*}
\centering
\includegraphics[width=16.4cm,clip]{1787f24.ps}
\caption{Plotted above...}
\label{appfig}
\end{figure*}

Because the optical images...
\end{appendix}

\begin{appendix} %Second online appendix
These studies, however, have faced...
\end{appendix}

\end{document}
%
%_____________________________________________________________
%        Some tables or figures are in the printed version and
%                      some are only in the electronic version
%-------------------------------------------------------------
%
% Leave all the tables or figures in the text, at their right place 
% and use the commands \onlfig{} and \onltab{}. These elements
% will be automatically placed at the end, in the section
% Online material.

\documentclass{aa}
...
\begin{document}
text of the paper...
\begin{figure*}%f1
\includegraphics[width=10.9cm]{1787f01.eps}
\caption{Shown in greyscale is a...}
\label{cl12301}}
\end{figure*}
...
from the intrinsic ellipticity distribution.
% Figure 2 available electronically only
\onlfig{
\begin{figure*}%f2
\includegraphics[width=11.6cm]{1787f02.eps}
\caption {Shown in greyscale...}
\label{cl1018}
\end{figure*}
}

% Figure 3 available electronically only
\onlfig{
\begin{figure*}%f3
\includegraphics[width=11.2cm]{1787f03.eps}
\caption{Shown in panels...}
\label{cl1059}
\end{figure*}
}

\begin{figure*}%f4
\includegraphics[width=10.9cm]{1787f04.eps}
\caption{Shown in greyscale is...}
\label{cl1232}}
\end{figure*}

\begin{table}%t1
\caption{Complexes characterisation.}\label{starbursts}
\centering
\begin{tabular}{lccc}
\hline \hline
Complex & $F_{60}$ & 8.6 &  No. of  \\
...
\hline
\end{tabular}
\end{table}
The second method produces...

% Figure 5 available electronically only
\onlfig{
\begin{figure*}%f5
\includegraphics[width=11.2cm]{1787f05.eps}
\caption{Shown in panels...}
\label{cl1238}}
\end{figure*}
}

As can be seen, in general the deeper...
% Table 2 available electronically only
\onltab{
\begin{table*}%t2
\caption{List of the LMC stellar complexes...}\label{Properties}
\centering
\begin{tabular}{lccccccccc}
\hline  \hline
Stellar & RA & Dec & ...
...
\hline
\end{tabular}
\end{table*}
}

% Table 3 available electronically only
\onltab{
\begin{table*}%t3
\caption{List of the derived...}\label{IrasFluxes}
\centering
\begin{tabular}{lcccccccccc}
\hline \hline
Stellar & $f12$ & $L12$ &...
...
\hline
\end{tabular}
\end{table*}
}
%
%-------------------------------------------------------------
%     For the online material, table longer than a single page
%                 In the preamble for landscape case, use : 
%                                          \usepackage{lscape}
%-------------------------------------------------------------
\documentclass{aa}
\usepackage[varg]{txfonts}
\usepackage{graphicx}
\usepackage{lscape}

\begin{document}
text of the paper
% Table will be print automatically at the end, in the section Online material.
\onllongtab{
\begin{longtable}{lrcrrrrrrrrl}
\caption{Line data and abundances ...}\\
\hline
\hline
Def & mol & Ion & $\lambda$ & $\chi$ & $\log gf$ & N & e &  rad & $\delta$ & $\delta$ 
red & References \\
\hline
\endfirsthead
\caption{Continued.} \\
\hline
Def & mol & Ion & $\lambda$ & $\chi$ & $\log gf$ & B & C &  rad & $\delta$ & $\delta$ 
red & References \\
\hline
\endhead
\hline
\endfoot
\hline
\endlastfoot
A & CH & 1 &3638 & 0.002 & $-$2.551 &  &  &  & $-$150 & 150 &  Jorgensen et al. (1996) \\                    
\end{longtable}
}% End onllongtab

% Or for landscape, large table:

\onllongtab{
\begin{landscape}
\begin{longtable}{lrcrrrrrrrrl}
...
\end{longtable}
\end{landscape}
}% End onllongtab
